# Anomaly Detection Project (UCL)
---

## üìë Table of Contents

- [Overview](#-overview)  
- [Project Timeline](#-project-timeline)  
- [Week 1: Stochastic Differential Equations (SDEs)](#-week-1-stochastic-differential-equations-sdes)  
- [Week 2: Anomaly Detection For SDEs](#-week-2-anomaly-detection-for-sdes)  
- [Week 3: Anomaly Detection For Fake Audio](#-week-3-anomaly-detection-for-fake-audio)  
  - [Step 1: Visual Exploration](#step-1-visual-exploration)  
  - [Step 2: Dataset Construction](#step-2-dataset-construction)  
  - [Step 3: Feature Extraction](#step-3-feature-extraction)  
  - [Step 4: Classification Algorithm](#step-4-classification-algorithm)  
  - [Step 5: Performance Evaluation](#step-5-performance-evaluation)  
- [Applications](#-applications)

---
## üìå Overview

Generative AI technologies can be maliciously used for misinformation, making detection tools crucial to keep up with the rapid evolution of synthetic content. This project frames the detection of fake audio as an **anomaly detection problem**. By leveraging **Stochastic Differential Equation (SDE) modeling** and **interpretable machine learning**, the goal is to develop a robust pipeline for identifying synthetic audio in time series data.

[Back to Top](#-table-of-contents)

---

## üìÖ Project Timeline

| Week | Activity |
|------|----------|
| 1    | Stochastic Differential Equations (SDEs) |
| 2    | Anomaly Detection for SDEs |
| 3    | Detecting Fake Audios & Presentation |

[Back to Top](#-table-of-contents)

---

## üìà Week 1: Stochastic Differential Equations (SDEs)

### What are SDEs?

A **Stochastic Differential Equation (SDE)** is like an ordinary differential equation (ODE) 
but with an additional noise term that models stochastic (random) fluctuations.

The computational definition of an SDE with Gaussian noise is:

<img width="591" height="56" alt="image" src="https://github.com/user-attachments/assets/c68e00b9-a7f7-46d6-bc15-283cb222246c" />


where Œæ is a normally distributed random variable with zero mean and unit variance. 
The value of X(t + Œît) is computed from X(t), and this process is repeated for each time step.

Formally, an SDE can be written as:

<img width="367" height="45" alt="image" src="https://github.com/user-attachments/assets/9346705f-0cdc-4072-9fd4-19e6ac199974" />


- f(X‚Çú, t) ‚Äî drift term (deterministic trend)  
- g(X‚Çú, t) ‚Äî diffusion term (magnitude of stochastic effects)  
- W‚Çú ‚Äî Wiener process (Brownian motion)  
- dW‚Çú ‚Äî infinitesimal random increment with mean 0 and variance dt

An **SDE realisation** is a single sample path generated by solving the SDE numerically, 
typically using methods like **Euler‚ÄìMaruyama**. Each realisation reflects a possible evolution of 
the process over time.

<img width="772" height="493" alt="image" src="https://github.com/user-attachments/assets/4032a2b4-9ab5-4bbd-b1b9-7e65bbd5388b" />

[Back to Top](#-table-of-contents)

---

## üîç Week 2: Anomaly Detection For SDEs
### Overview
Design an algorithm that can classify sample paths (realisations) from these three different SDEs.

<img width="800" alt="download" src="https://github.com/user-attachments/assets/7ba48e1c-def3-4b74-a79b-3783c96f4b62" />

### Data
- **Corpus:** 10,000 SDE 1 realisations  
- **Validation:** 2,000 SDE 1, 1,000 SDE 2, 1,000 SDE 3  
- **Test:** Same as validation, shuffled  

### Approach

1. **Visual Exploration**  
   - Plotted realisations of the three SDEs and explored feature distributions (scatter plots, histograms) to identify separations.  
   - Example of SDE realisations:  
     <img width="800" alt="Figure_1" src="https://github.com/user-attachments/assets/965da53b-5944-477a-91a9-7eef2795f7b2" />

   - Explored features to see which gave greatest separation between SDEs.

2. **Observations**  
   - **SDE 2:** highest noise ‚Üí largest standard deviation of increments  
   - **SDE 1:** intermediate standard deviation of increments
   - **SDE 3:** filtered ‚Üí smallest standard deviation of increments  
   - Clear separation observed between **SDE 1 and SDE 2**, partial overlap between **SDE 1 and SDE 3**  
     <img width="640" height="480" alt="image" src="https://github.com/user-attachments/assets/415f8d61-3ee9-4abf-b5bb-bec10644aaa3" />


3. **Classification**  
   - Manually chose thresholds based on observed standard deviation of increments.  
   - Applied thresholds to classify each path.

4. **Evaluation**  
   - Increasing the number of timestamps ‚Üí greater accuracy 
      <img width="800" alt="Accuracy vs Timestamps" src="https://github.com/user-attachments/assets/2209a191-9500-4a65-88fe-fa09aada9471" />

   - Increasing sample size ‚Üí tends to the real accuracy
      <img width="800" alt="Sample Size vs Accuracy" src="https://github.com/user-attachments/assets/52f12f26-f78d-4ac7-b4c1-1fc705de7a20" />

   - Additional feature analysis could further improve the algorithm.

[Back to Top](#-table-of-contents)

---

## üéß Week 3: Anomaly Detection For Fake Audio

### Overview
Goal: Build an algorithm to classify short audio clips as **real or fake**.

Given
- 12 full audiobook chapter recordings (real audio)  
- 24 fake clips from Chapter 1  
- 5 fake clips from Chapter 4  
- 7 fake clips from Chapter 5

‚ö†Ô∏è Note: There are significantly fewer fake audio clips than real audio clips. This was intentionally set as it reflects real-world conditions where fake data is much less common than real data.

[Back to Top](#-table-of-contents)

---

### Step 1: Visual Exploration
- Generated **Mel spectrograms** for visual inspection  
- Analysed **waveforms, frequency distributions**, and other audio features
  
   <img width="800" alt="Figure_1" src="https://github.com/user-attachments/assets/7d896758-8f4a-4bfe-bbd8-048cce4d84a4" />

[Back to Top](#-table-of-contents)

---
### Step 2: Dataset Construction
- Real audio was split into ~30-second chunks based on pauses in speech and converted into JSON metadata for each segment which is stored in `anthem_transcriptions.json`.
- Created folders:
  - **Corpus (70% real)** ‚Üí 150 clips  
  - **Validation (15% real + 50% fake)** ‚Üí 46 clips  
  - **Test (15% real + 50% fake)** ‚Üí 44 clips  
- Stratified sampling ensured balanced chapter representation in validation and test sets

[Back to Top](#-table-of-contents)

---
### Step 3: Feature Extraction
- Extracted 40+ audio features for each clip such as:  
  - **Spectral Centroid (mean/std/range)** ‚Äì indicates where the ‚Äúcenter of mass‚Äù of the spectrum is, related to perceived brightness of sound  
  - **Spectral Bandwidth (mean/std)** ‚Äì measures the spread of frequencies around the centroid, describing timbre width  
  - **Spectral Flatness (mean/std)** ‚Äì quantifies noisiness vs tonal quality of the signal  
  - **Spectral Rolloff (mean/std)** ‚Äì frequency below which a specified percentage of energy is contained, used to separate harmonic vs noisy signals  
  - **Zero-Crossing Rate (mean/std, per second)** ‚Äì counts sign changes in waveform, capturing signal noisiness or activity  
  - **Speech Rate** ‚Äì number of detected onsets per second, approximating spoken pace  
  - **MFCCs (means/stds)** ‚Äì Mel-frequency cepstral coefficients capturing timbral characteristics and overall spectral shape  

- Constructed dataframes for **corpus, validation, and test clips** and stored in a `audio_features_dataset.csv` csv file for clean algorithm implementation

[Back to Top](#-table-of-contents)

---

### Step 4: Classification Algorithm

--- 

### Non-ML Approach
- Compute **mean (Œº)** and **std (œÉ)** for each feature from corpus  
- For each clip, calculate **Z-scores** of features using:
  
   <img width="168" height="62" alt="image" src="https://github.com/user-attachments/assets/62c8638d-edfb-4151-b2b6-1ec15447a3e0" />
   
- Mark features as abnormal if Z-score > tuned threshold  
- Clip classified as **Fake** if number of abnormal features > tuned threshold  
- Thresholds tuned using **validation set**:
  - **Z-score threshold** ‚Üí how much deviation counts as abnormal  
  - **Feature count threshold** ‚Üí how many abnormal features indicate fake  
- More abnormal features ‚Üí lower confidence clip is real

<img width="800" alt="Dist of Abnormal Feature Counts" src="https://github.com/user-attachments/assets/809022b5-b416-4a99-b846-a2bb8eded039" />
<img src="https://github.com/user-attachments/assets/8111e093-a861-4640-bb79-226a6cb8ee94" alt="F1-Based Tuning Heatmap" width="800" />

<img src="https://github.com/user-attachments/assets/e2c40c18-11c3-4ba5-9651-2d40c19ce642" alt="PDF Distributions" width="800" />


---

### ML Approach (Isolation Forest)

Isolation Forest is an **unsupervised anomaly detection machine learning algorithm**.
- It is an **ensemble method**, like a Random Forest, meaning that it combines the results of multiple trees to compute a final anomaly score.
- Unlike other methods that define what‚Äôs ‚Äúnormal‚Äù first, Isolation Forest directly isolates points that are different from the rest.

### How It Works
1. **Randomly select a feature** (dimension) and a value within its range.  
2. **Split the data** along that value, creating two subgroups.  
3. **Recursively repeat** this process to build a tree.  
4. Each **leaf node** eventually contains a single data point.  
5. **Outliers (anomalies)** are points that get isolated **quickly** in the tree (at a smaller depth).  
6. Multiple trees are combined into a **forest**, and the final anomaly score for a point is the **average depth across all trees**.

### Intuition

- Normal points are **grouped together** and require more splits to isolate.  
- Anomalies are **few and different**, so they are **isolated faster**.

### Implementation
- **Features scaled** using `StandardScaler` to ensure that all features contribute equally to the distance computations in the algorithm.
- The Isolation Forest is trained on the scaled training set from the corpus. This allows the model to learn the structure of 'normal' data
- Hyperparameters are tuned on the validation set to maximize the F1-score. The hyperparameters considered include:
   - `contamination`: expected proportion of anomalies in the data
   - `n_estimators`: number of trees in the forest
   - `max_samples`: fraction of samples to draw for each tree
   - `max_features`: fraction of features to draw for each tree
- The model with the highest F1-score on the validation set is selected as `best_model` to be used on the test set.

[Back to Top](#-table-of-contents)

---

### Step 5: Performance Evaluation

---
### Non-ML Approach

- Model Evaluation: Predictions are assessed using accuracy, classification metrics, and a confusion matrix.
   <img width="640" height="480" alt="Confusion Matrix" src="https://github.com/user-attachments/assets/30b52943-cfea-4db7-84a0-07a9e7bd1f13" />
   
- Feature Importance Analysis: Feature importance is plotted which ranks the features that are more frequently abnormal in fake audio compared to real audio (i.e. which features best separate fake audio from real audio)
   <img width="800" height="800" alt="Feature Importance (Non-ML)" src="https://github.com/user-attachments/assets/5683b589-1471-4c68-b0e9-ed3c557dc206" />

- Insights for Improvement: Removing consistently negative or irrelevant features may help reduce error and improve overall model performance. Other more advanced audio features could've been extracted that could help seperate real and fake audio even better.
---
### ML Approach

- Model Evaluation: Model performance is evaluated on the test set using accuracy, classification metrics and a confusion matrix:
<img width="600" height="400" alt="Isolation Forest Confusion Matrix" src="https://github.com/user-attachments/assets/474474ef-3446-4815-8a7a-fb698c7beec7" />

- Feature Importance Analysis: SHAP values are used to quantify each feature‚Äôs contribution to correct or incorrect predictions. Features with positive contributions indicate strong separation between real and fake audio, while negative contributions indicate misleading or less informative features. As seen below, there are common features that performed well (or worse) between the ML and Non-ML algorithms.
   <img width="800" alt="Feature Importance (ML)" src="https://github.com/user-attachments/assets/0ba8c755-84a9-46e4-97ad-b0f7fef2ae44" />

- Insights for Improvement: Similarly to the Non-ML approach, removing consistently negative or irrelevant features may reduce error and improve model performance. Other more advanced audio features could've been extracted that could help seperate real and fake audio even better.


[Back to Top](#-table-of-contents)

---

üöÄ Applications

The techniques and insights from this project can be applied across various domains:

üí∞ **Finance:** Fraud detection, identifying anomalous trading patterns, or detecting synthetic market data.

ü©∫ **Healthcare:** Detecting anomalies in medical signals like ECG, EEG, or imaging data for early diagnosis.

üõ°Ô∏è **Cybersecurity:** Identifying malicious or synthetic network traffic, phishing audio, or deepfake attacks.

üè≠ **Manufacturing & IoT:** Predictive maintenance by detecting abnormal sensor readings in machinery or IoT devices.

‚úàÔ∏è **Aerospace & Transportation:** Monitoring engine or system performance for anomalies to prevent failures.

üéß **Audio, Speech & Multimedia:** Detecting deepfake audio, voice cloning, or tampered media content.

üì± **Social Media:** Identifying synthetic posts, audio, or videos to combat misinformation.

üåç **Environment:** Monitoring sensor networks for anomalous environmental readings or climate data patterns.

[Back to Top](#-table-of-contents)

---

End of Project
