# Anomaly Detection Project (UCL)
---

## üìå Overview

Generative AI technologies can be maliciously used for misinformation, making detection tools crucial to keep up with the rapid evolution of synthetic content. This project frames the detection of fake audio as an **anomaly detection problem**. By leveraging **Stochastic Differential Equation (SDE) modeling** and **interpretable machine learning**, the goal is to develop a robust pipeline for identifying synthetic audio in time series data.

---

## üìÖ Project Timeline

| Week | Activity |
|------|----------|
| 1    | Stochastic Differential Equations (SDEs) |
| 2    | Anomaly Detection for SDEs |
| 3    | Detecting Fake Audios & Presentation |

---

## üìà Week 1: Stochastic Differential Equations (SDEs)

### What are SDEs?

A **Stochastic Differential Equation (SDE)** is like an ordinary differential equation (ODE) 
but with an additional noise term that models stochastic (random) fluctuations.

The computational definition of an SDE with Gaussian noise is:

<img width="591" height="56" alt="image" src="https://github.com/user-attachments/assets/c68e00b9-a7f7-46d6-bc15-283cb222246c" />


where Œæ is a normally distributed random variable with zero mean and unit variance. 
The value of X(t + Œît) is computed from X(t), and this process is repeated for each time step.

Formally, an SDE can be written as:

<img width="367" height="45" alt="image" src="https://github.com/user-attachments/assets/9346705f-0cdc-4072-9fd4-19e6ac199974" />


- f(X‚Çú, t) ‚Äî drift term (deterministic trend)  
- g(X‚Çú, t) ‚Äî diffusion term (magnitude of stochastic effects)  
- W‚Çú ‚Äî Wiener process (Brownian motion)  
- dW‚Çú ‚Äî infinitesimal random increment with mean 0 and variance dt

An **SDE realisation** is a single sample path generated by solving the SDE numerically, 
typically using methods like **Euler‚ÄìMaruyama**. Each realisation reflects a possible evolution of 
the process over time.

<img width="772" height="493" alt="image" src="https://github.com/user-attachments/assets/4032a2b4-9ab5-4bbd-b1b9-7e65bbd5388b" />

---

## üîç Week 2: Anomaly Detection For SDEs
### Overview
Design an algorithm that can classify sample paths (realisations) from these three different SDEs.

<img width="1360" height="556" alt="download" src="https://github.com/user-attachments/assets/7ba48e1c-def3-4b74-a79b-3783c96f4b62" />

### Data
- **Corpus:** 10,000 SDE 1 realisations  
- **Validation:** 2,000 SDE 1, 1,000 SDE 2, 1,000 SDE 3  
- **Test:** Same as validation, shuffled  

### Approach

1. **Visual Exploration**  
   - Plotted realisations of the three SDEs and explored feature distributions (scatter plots, histograms) to identify separations.  
   - Example of SDE realisations:  
     <img width="1000" height="600" alt="Figure_1" src="https://github.com/user-attachments/assets/965da53b-5944-477a-91a9-7eef2795f7b2" />

   - Explored features to see which gave greatest separation between SDEs.

2. **Observations**  
   - **SDE 2:** highest noise ‚Üí largest standard deviation of increments  
   - **SDE 1:** intermediate standard deviation of increments
   - **SDE 3:** filtered ‚Üí smallest standard deviation of increments  
   - Clear separation observed between **SDE 1 and SDE 2**, partial overlap between **SDE 1 and SDE 3**  
     <img width="640" height="480" alt="image" src="https://github.com/user-attachments/assets/415f8d61-3ee9-4abf-b5bb-bec10644aaa3" />


3. **Classification**  
   - Manually chose thresholds based on observed standard deviation of increments.  
   - Applied thresholds to classify each path.

4. **Evaluation**  
   - Increasing the number of timestamps ‚Üí greater accuracy 
      <img width="1000" height="500" alt="Accuracy vs Timestamps" src="https://github.com/user-attachments/assets/2209a191-9500-4a65-88fe-fa09aada9471" />

   - Increasing sample size ‚Üí tends to the real accuracy
      <img width="1000" height="600" alt="Sample Size vs Accuracy" src="https://github.com/user-attachments/assets/52f12f26-f78d-4ac7-b4c1-1fc705de7a20" />

   - Additional feature analysis could further improve the algorithm.

---

## üéß Week 3: Anomaly Detection For Fake Audio

### Overview
Goal: Build an algorithm to classify short audio clips as **real or fake**.

Given
- 12 full audiobook chapter recordings (real audio)  
- 24 fake clips from Chapter 1  
- 5 fake clips from Chapter 4  
- 7 fake clips from Chapter 5

‚ö†Ô∏è Note: There are significantly fewer fake audio clips than real audio clips. This was intentionally set as it reflects real-world conditions where fake data is much less common than real data.

---

### Step 1: Visual Exploration
- Generated **Mel spectrograms** for visual inspection  
- Analysed **waveforms, frequency distributions**, and other audio features
  
   <img width="1442" height="842" alt="Figure_1" src="https://github.com/user-attachments/assets/7d896758-8f4a-4bfe-bbd8-048cce4d84a4" />

---
### Step 2: Dataset Construction
- Real audio was split into ~30-second chunks based on pauses in speech and converted into JSON metadata for each segment which is stored in `anthem_transcriptions.json`.
- Created folders:
  - **Corpus (70% real)** ‚Üí 150 clips  
  - **Validation (15% real + 50% fake)** ‚Üí 46 clips  
  - **Test (15% real + 50% fake)** ‚Üí 44 clips  
- Stratified sampling ensured balanced chapter representation in validation and test sets  

---
### Step 3: Feature Extraction
- Extracted 40+ audio features for each clip such as:  
  - **Spectral Centroid (mean/std/range)** ‚Äì indicates where the ‚Äúcenter of mass‚Äù of the spectrum is, related to perceived brightness of sound  
  - **Spectral Bandwidth (mean/std)** ‚Äì measures the spread of frequencies around the centroid, describing timbre width  
  - **Spectral Flatness (mean/std)** ‚Äì quantifies noisiness vs tonal quality of the signal  
  - **Spectral Rolloff (mean/std)** ‚Äì frequency below which a specified percentage of energy is contained, used to separate harmonic vs noisy signals  
  - **Zero-Crossing Rate (mean/std, per second)** ‚Äì counts sign changes in waveform, capturing signal noisiness or activity  
  - **Speech Rate** ‚Äì number of detected onsets per second, approximating spoken pace  
  - **MFCCs (means/stds)** ‚Äì Mel-frequency cepstral coefficients capturing timbral characteristics and overall spectral shape  

- Constructed dataframes for **corpus, validation, and test clips** and stored in a `audio_features_dataset.csv` csv file for clean algorithm implementation

---

### Step 4: Classification Algorithm

--- 

### Non-ML Approach
- Compute **mean (Œº)** and **std (œÉ)** for each feature from corpus  
- For each clip, calculate **Z-scores** of features using:
  
   <img width="168" height="62" alt="image" src="https://github.com/user-attachments/assets/62c8638d-edfb-4151-b2b6-1ec15447a3e0" />
   
- Mark features as abnormal if Z-score > tuned threshold  
- Clip classified as **Fake** if number of abnormal features > tuned threshold  
- Thresholds tuned using **validation set**:
  - **Z-score threshold** ‚Üí how much deviation counts as abnormal  
  - **Feature count threshold** ‚Üí how many abnormal features indicate fake  
- More abnormal features ‚Üí lower confidence clip is real
---

### ML Approach (Isolation Forest)

Isolation Forest is an **unsupervised anomaly detection machine learning algorithm**.
- It is an **ensemble method**, like a Random Forest, meaning that it combines the results of multiple trees to compute a final anomaly score.
- Unlike other methods that define what‚Äôs ‚Äúnormal‚Äù first, Isolation Forest directly isolates points that are different from the rest.

### How It Works
1. **Randomly select a feature** (dimension) and a value within its range.  
2. **Split the data** along that value, creating two subgroups.  
3. **Recursively repeat** this process to build a tree.  
4. Each **leaf node** eventually contains a single data point.  
5. **Outliers (anomalies)** are points that get isolated **quickly** in the tree (at a smaller depth).  
6. Multiple trees are combined into a **forest**, and the final anomaly score for a point is the **average depth across all trees**.

### Intuition

- Normal points are **grouped together** and require more splits to isolate.  
- Anomalies are **few and different**, so they are **isolated faster**.

### Implementation
- **Features scaled** using `StandardScaler` to ensure that all features contribute equally to the distance computations in the algorithm.
- The Isolation Forest is trained on the scaled training set from the corpus. This allows the model to learn the structure of 'normal' data
- Hyperparameters are tuned on the validation set to maximize the F1-score. The hyperparameters considered include:
   - `contamination`: expected proportion of anomalies in the data
   - `n_estimators`: number of trees in the forest
   - `max_samples`: fraction of samples to draw for each tree
   - `max_features`: fraction of features to draw for each tree
- The model with the highest F1-score on the validation set is selected as `best_model` to be used on the test set.
  
---


